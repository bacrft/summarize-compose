The document is about the power of collective intelligence and its reciprocal challenges. 
-The president of the academy, Olivier Appert, is a member of the Académie des technologies. 
-Richard Lavergne, co-president, Denis Randet, co-president are also members of the academy. 
-The document was written by members of the group working on "Alais," which is a project led by Jean-Christophe Air Liquide. 
-The work is supported financially by souscriptors to FutuRIS: AI CARNOT, AIR LIQUIDE, AMPIRIC - AIX-MARSEILLE UNIVERSITE, ANR, BERGER-LEVRAULT, BOUYGUES, BRGM, CEA, CNRS, CPU, EDF , ENGIE , FACEBOOK , GE HAELTHCARE , INSERM , INSTITUT MINES TELECOM , INRIA , INSTITUT PASTEUR , IRIS SERVIER , MINISTERE DE L’EDUCATION NATIONALE ET DE LA JEUNESSE , MINISTERE DE L’ENSEIGNEMENT SUPERIEUR , DE LA RECHERCHE ET DE L’INNOVATION , NOKIA , REGION PAYS DE LA LOIRE,, SHNEIDER ELECTRIC,, SNCF,, TOTAL ENERGIES The document discusses the issue of energy consumption in the digital age, and how physical and technological limits are affecting the way we use energy.
- The article discusses how new computer architectures are being developed to address these limitations, such as the FOG computing model used for 5G telecommunications. It also looks at how processors are becoming more specialized and parallelized, and how data centers are becoming more efficient.
- The document also touches on security and resilience issues related to our increasingly digital world. The achievements of OVH 27 1-4/ The super computers, the production of Atos 28 1-5/ The quantum program 28 1-6/ Telecommunications 29 1-7/ Energy control 

1. The group's work in 2021-2022 focused on the study of disasters, which included businesses and public bodies. They did not take into account the advances made in technology since 2017, when it was organized by ANRT under the presidency of Olivier Appert. 
2. The problem remains but our research shows that it is less relevant in application areas than in technological issues related to digital transformation itself. Our focus is on energy transition pragmatically, taking into account that these sectors are very dependent on questions about energy supply. Everywhere, the internet is a means to improve productivity and reduce energy consumption particularly for its use in electric networks modified by technology. "The introduction in the primary electricity sector of renewable intermittent energies. Globally, these technologies constitute one of Certainly, if electric consumption is still a lever for fighting climate change, an element of the cost of ownership, in manufacturing and environmental impact. However, as the Paris Agreement on Climate Change implies a reduction but this is not a blocking point. emissions of CO by -7.6% per year by 2025" 
- "It is important to analyze the digital footprint: now, the environmental dissipation of heat throughout all sectors of activity limits performance, since systems throughout the cycle life. And digital integrated to computers. And as no escape: law Moore and its continuing pursuit of miniaturization, we need to find electric consumption and emissions of other means."
- "It is necessary to challenge traditional separation between materials upstream from the cycle are also subjects traditional between manufacturing process processors vigilance." It is essential that gains induced worldwide be not canceled by digital systems. For example, in future cars The digital world is being designed by manufacturers who are skilled in cholesterol: there is the good and the bad. 
- This year, the group has focused on accessing - while managing their confidential information consumption - powerful supercomputers capable of modeling intelligence problems, which are unquestionable, since computer growth seems to have no limit. 
- 6

This is a revolution of value chains. Our work gives an idea for some industrial sectors, but it will be true for all. This revolution can open opportunities, but risks giving control of our industry to countries that will master the new interaction between electronics and products. France and Europe face a double problem: 
- Managing the entire chain of digital technology; 
- That application sectors have competent personnel to design - in conjunction with European electronic industry - the digital technology used in product conception, function and maintenance. "The technologies have improved a lot." 
- This has refuted the catastrophic predictions, such as one that appeared in Forbes in 1999, according to which half of American electricity production would be absorbed by the digital economy and the Internet. 
- In reality, the predictions are difficult to make, because of the rapid change in demand, the efficiency of TICs, and how they are used. 
- Methods and assumptions differ depending on organizations. Extreme results that are communicated a lot result from simplistic models. For example, for data center consumption (Figure 1), these models give results that are two times higher than probable (200-350 TWh). 
- In France, according to a report from the Senate, digital technology had an energy footprint of 15 Mt in 2019 - or 2% of France's total carbon footprint - with terminals representing 81% of this figure. The carbon footprint of digital technology in France.
- There are voluntarist scenarios proposed by, for example, the ADEME or the Ministry of Ecological Transition.
- It depends on several factors, such as the choices made for 5G coverage, the development of Internet of Things and edge computing. 
- The General Economic Council has made a 2030 projection, extrapolating ARCEP data. 
- In 2030, global reductions are expected but consumption of data centers and networks is increasing significantly. 
- One particular issue is video resolution which would increase consumption by 10% if it transitioned to 4K. However, energy consumption measures are indirect because information technologies have not been equipped to be measured directly. 
- Another issue is equipment manufacturing which accounts for 40% of the overall impact of digital technology today. Unfortunately there are few in depth studies on this topic, if any at all other than an academic study "Comme ils sont fabriqués en grande partie hors d’Europe, nous n’avons guère de moyens d’action directs, si ce n’est la diffusion de labels indiquant l’empreinte carbone."
- "Les progrès en durabilité des équipements réduisent l’impact de leur fabrication."
- "De même il convient de favoriser la réparabilité et le recyclage."
- "Des réglementations et normes existent, notamment sur l’obsolescence programmée"
- "Une autre partie concerne la limitation des ressources consommées grâce à une loi sur l’économie circulaire."
- "Le point à relever est que les données précises concernant les équipements et leur recyclage semblent manquer dans le domaine du numérique." The document discusses the increasing traffic and energy consumption, the lack of reliable data on digital energy use due to an evolving methodology, and the large overall impact of digital technologies.
-The manufacture of electronic equipment represents today 40% of the global impact of digital technologies.
-There is a great divergence in projections because different estimates are made about productivity gains. The evacuating heat to the thermostat is greater than the information acquired by the system's processor. The thermodynamic processor, which realizes this acquisition by increasing entropy, consumes energy from noble sources (electricity), and dissipates it in a thermostat maintaining overall energy transfers, according to the first principle of thermodynamics. Its dual machine is Szillard's engine (1929), and thermodynamic classification suggests seeing the processor as a refrigerator whose efficiency is defined by a performance coefficient. This last is useful for calculating thermal service rendered by digitalization at the end of an elementary cycle of function operation. To obtain the processor for this calculation, its cycle of function operation "closes" on a mechanism for erasing information (Landauer's paradigm4) that allows returning information's value to energy allocated to it by electronic logic circuit polarization: • currently, CMOS technology can affect memory with an efficiency around 10-5; • with electronical spin or spintronic4 technology, this efficiency would increase 2 or 3 orders of magnitude if this technology were not yet capable of providing a complete computer.
3- Formulated for the first time in 1961 by R. Landauer at IBM, the minimal energy level required to erase one bit of information stands at kB TLn2 where kB is Boltzmann's constant and T temperature The physical system considered. This postulated empirically has been confirmed experimentally in 2012 by an team of the University of Lyon.
- The spintronic or electronic spin control offers information control on the spin of electron (by applying a magnetic field) and reading information through the charge current of electron.
- Physically, since the advent of CMOS technology and microprocessor (1960), progress made in integration transistor and increase in gravure resolution has improved processor performance to such an extent that: 
- Between 1971 and 2001, transistor density doubled every 1.96 years; 
- Between 1946 and 2009, calculations per Joule spent doubled every 1.57 years. As a result, computers have become smaller while becoming faster and more powerful than ever before, but the Landauer paradigm which sets a minimum energy required to erase an elementary information (2.80 zJ at ambient temperature) constitutes a physical limit to Moore’s law. Koomey doesn't extrapolation should be reached by the end of 2020 with the current computer architecture.
- The limits of Moore's law are described below. In practice, the first signs of exhaustion of Moore's and Koomey laws manifest themselves in the middle of the 2000s - 2010. To push these limits, other paradigms for calculation were proposed, such as in 1973 by C. Bennett from IBM, which rely on a distinction between physical and computational entropy: 
- Physical entropy is associated with fluctuations in a system macroscopically to reach equiprobability of its microstates; heat generated is related to variations in these fluctuations when the system evolves and irreversibility is due to an imposed evolution over a finite time period; 
- Information entropy is associated with loss of information; half of losses are due to irreversibility (erasure of memory entries), while the other half is due to dissipation of circuits in commutation among transistors polarized with voltage levels spaced at intervals. In this context, one could envisage: 
- slowing down processors by polarizing them with slow ramping voltages; 
- provided that enough memory is available, retaining intermediate steps during computation so that "reversible rebooting" can be performed and energy associated with erasing mechanisms can be recovered. To achieve this result within a comparable time frame, parallelization would be adopted. While the possibility to realize reversible computation results from a compromise between material traces Le calcul quantique est caractérisé par son « état », qui est représenté par un vecteur immobile sur un espace vectoriel de Hilbert. 
- Depuis plus de quarante ans, la loi dite de Moore pousse à l’ordinateur quantique en raison du risque que le numérique se heurte au mur de la puissance des microprocesseurs. Cette loi empirique résulte du fait que les fabricants semi-conducteurs ont déjà doublé la densité de transistors gravés sur le corps des silicium complexes durant les années passées. Toutefois, les progrès possibles en matière de miniaturisation des transistors sont confrontés à des difficultés considérables dues aux limites imposées par la puissance dissipée et à la densité d’intégration valable tant qu’on ne mesure rien. The technology of 5nm) imposes on stacking precisely these properties, with more and more levels of metallization for the probability of square modules of component parts to be interconnected, which tends to create a vector on the basis of these property vectors. 
-To partially offset the advantage that increasing measure has over this increase in performance, it will require if we were to achieve the same result by increasing performance by the same amount; this is guaranteed due to delays in signal propagation due to the fact that when one measures, the vector E circuits. The impossibility of continuing to reduce as it was disappears and becomes the own vector regularly in 2D dimensions critical dimensions corresponding to the property value that was measured. 
- lithography requires innovative solutions for integration in three dimensions An electronic quantum computer is manufactured from transistors. Additionally, industry facing semi-quantum bits. A qbit is a quantum system consisting of two conductors located today at one physical limit: transistors at nanoscale approach size atomic nucleus (+>). Two (0.1 nm). At this scale, the behavior of qbits are represented by a vector in an space particles physics described by non-deterministic laws: a/++> + b/+-> + c/-+> + d/-->. Which put into effect The "defect" is called "intricate." Although, for example, 2 bits per 10 bits in terms of chip evolution advances for several decades, miniaturization of the space to 210=1024 dimensions has been sufficient to stimulate innovation, allowing inventing new usages and ensuring an algorithm will evolve this vector of state. Thus, the growth of the industry of semi-conductor conductors must now build them by-passing the law of Moore. A radiofrequency impulse such as those used in nuclear magnetic resonance, on one or more bits. At the end of its execution, one "goes to result" by measuring something.

-The advantage of quantum over classical computing is that each operation is performed on the state vector in its entirety, quantum-computing two main reasons why all states base can be affected at once. 
-One reason is that it confirms that quantum calculation would be less energy consuming than classical calculation. The first reason is that it requires much fewer steps to get from a phone number's digits to a name; the second reason is that "portals" are reversible and transfer only a recoverable energy Oracle allows subscribers to change their sign today. 
-The subscriber who originally provided their phone number. 
-There is then a series of quantum calculations that would theoretically allow: 
-Cryptography: algorithm of Shor; inverse (following the algorithm of Grover). 
-Maximization of very large numbers (physics: algorithmic scheme of Hartle); being inversed (conclusion of the algorithm of Grover). 
-Finding the minimum energy state for a complex molecule or organism; optimizing systems very complex: parallelizing on two computers, or in specialized cases only. 
Physical particles have ... ; accelerating computer quantum physics performance; using the computer quantum for its performance. The document discusses the parameters of information systems, including the number of operations per second (OPS) and the power to process.
-The number of integrated circuits per processor has been increasing steadily over time, but it is now capped at 4-5 GHz. 
-The amount of software that can be processed simultaneously decreases when specialized processors are used. 
-Parallel processing wastes up to 90% of existing software. Traffic, or throughput, expressed in simultaneous messages.This is related to the calculation method and the organization of the system. Operations that require a large power cannot be handled on regular computers, at least not without some kind of group calculation: • a part of traffic is the communication between these computers and terminals, phones, communication devices...; • another part is the consequence of using an distributed algorithm like blockchain, which wants to replace central validation with user validation; • another part is streaming video. It grows with the definition of images. Is 4K enough? 16INFRASTRUCTURES
3. Latency, or time it takes for one message to be transmitted from one point to another. 
7. Architectures, compromises between centralized and decentralized information management. 
4. Energy consumption The compromise results in two kinds of choices: architectural ones with enclosure and network deployment • manufacturing and recycling of hardware. 
5G architecture has been defined with standards set by Europe (e.g., ETSI EN 302 716), while edge architecture tries to do as much processing locally as possible (e.g., FOG). 
5 years are still needed for most goals set by authorities 10 years ago: • establishment of clouds hierarchically arranged; • operation on local data instead of transferring it across networks every time it's needed;
• handling big data more efficiently by breaking it down into smaller pieces before analyzing it The two are difficult to measure: sensors-actuators, especially in self-driving cars; 
-The first because complete cycles of cars with autonomous driving; for these, the edge is hard to track, given that the bulk of production is in Asia, with an emission output strong CO; 
-However, models of artificial intelligence must be powered by a strongly emitting CO source; 
-Operational dependence on configurations of calculation varies constantly and without detectable change. This is a real research topic. 
5. Algorithmic complexity According to the quantity of data to be processed, this complexity is expressed in terms of memory size or number of operations executed by a computer program that uses an algorithm . When the algorithm is distributed , that is when it results from independent and asynchronous execution by actors communicating through messages , then the number of messages exchanged constitutes another criterion of complexity. 6. Platformization The storage of data in powerful computing devices (data centers) allows users access to processing power they could not afford on their own . And the combination of many data sets opens up new services. But platformization has its downside : ownership over data or at least rights to access it. To escape the grip of Gafa over their data, Europeans have decided to form GAIA-X system labeling them as such. Gafa are part of it thankfully, since wanting control over one’s data implies having control over everything The cost of passing their base would be too expensive. But it is the big European industrialists who will be the guarantors of our independence. The Germans have understood this. 
-There is always a compromise between quality of service and energy consumption. One can optimize by choosing the best path between different servers and networks. 
-In conclusion, the FOG computing brings great progress in telecommunications (IoTs, connected cars) which allows to specialize support. Furthermore, the complexity of the computer network has increased. The current architecture is in transition: we observe a challenge to put in place tools to measure this intermediate layer, FOG for understanding energy consumption of computing (million processors with an entire system). We will have to experiment and push local networks, connected with data centers, connection of measurement tools. And finally, smart devices manage connections in the control algorithms for SDN network with mobile devices, connected objects, sensors etc.). 
-It is fundamental to notice that FOG computing is performed with compact computers and this telecommunications architecture is reliable as NUCs from Intel which can also do everything Edge computing is a distributed system for managing network resources between acting as "SDN routers," interconnected several clouds interconnected. 
-The ability of local actors to manage the change in and small servers for storing data paradigm shift is a real question, as well as and offer services. 
-His role is to feed the competitiveness of traditional telcos against GAFA software supporting applications (mobile, domotics, security and surveillance, traffic management automotive), which at the end of the network are 2-2 / L’EDGE COMPUTING AND IOT, connected to "terminals" (captors, actuators LE CONTINUUM COMPUTING or mobile), while ensuring a relatively fast access to the system. 
-Currently FOG specializes in two areas: security and rapid processing - or preprocessing - can be carried out within edge computers close to where data is created. 
-Each FOG is an ensemble of different clients (who represent who differ especially by their location in terms of application production chains), as well as various purposes for which data may be used. The document discusses the treatment of a system, which should be remonted towards another part of the services. This structure also has a centralized cloud. For internal artificial intelligence network (outside of public Internet) in and automated learning, the development which there is then a controller SDN and several (training) model of phenomenon at NUC. The controller SDN creates connections to study is done centrally in dynamic way to treat the requests that are sent to the cloud (often via a supercomputer installed at its address). This effect), because it benefits both from a considerable computational power and the ability to control 
- Energy consumption is measured, for example, by crossing data sources very different in origin. In an NUC, measuring current. once the model is developed, it can be loaded into devices whose behavior will be subjected to system. During overloads, the time response model increases and the controller SDN adapts connections accordingly. 183-1 / L’INTELLIGENCE ARTIFICielLE AU SERVICE DES PROCESSUS INDUSTRIELS Les systèmes d’intelligence artificielle ont un rôle clé dans les processus industriels car ils permettent de réaliser des tâches complexes et automatisées qui étaient jusqu’alors réservées aux humains ou aux machines traditionnelles. Ces systèmes sont souvent utilisés pour améliorer la performance du processus industriel et faciliter son exploitation en termes de productivité et de qualité. Les applications les plus courantes concernent l’optimisation du travail en général et la gestion des stocks en particulier. 184-1 / LES SYSTÈMES D’INTELLIGENCE ARTIFICielLE AU SERVICE DES PROCESSUS INDUSTRIELS Les systèmes d’intelligence artificielle ont un rôle clé dans les processus industriels car ils permettent de réaliser des tâches complexes et automatisées qui étaient jusqu'alors réservées aux humains ou aux machines traditionnelles . Ces systèmes sont souvent utilisés pour améliorer la performance du processus industriel et faciliter son exploitation en termes de productivité et de qualité . Les applications les plus courantes concernent l'optimisation du travail en générale et la gestion des stocks en particulier . 185-2 / LES AVANTAGES DU SYSTÈME D'INTELLIGENCE ARTIFICielLE DANS LA GESTION DU TRAVAIL La gestion du travail est une tâche complexe qui requiert l’utilisation d’outils efficaces pour identifier les problèmes à venir , anticiper leur arrivée sur le lieu de travail afin d'y faire face avant qu'ils ne causent des dégâts irréparables , mettre au point une stratégie corrective appropri The document discusses ways to improve performance, and two possible paths are specialization of processors or parallel computing. The problem is that programming becomes more difficult as the application code is executed in a specialized processor module. In order to take advantage of parallel processing capabilities, an entire application software development infrastructure will need to be redeveloped. Programs written 30 years ago are no longer usable due to parallel processing limitations. 
-Parallelism can be implemented at different levels: 
-Processors 
-Integrated circuits that can group several processors (called "sockets" in American terminology) 
-Servers where multiple "sockets" can coexist 
-Large systems like supercomputers or cloud servers, which contain thousands of servers in local network The document discusses the learning process, which is usually referred to as hybrid models that are simpler physical models combined with model Larger dimensioning of data systems energy-related models.
-One way to improve efficiency is to measure energy consumption, climate profile of a site) and energy-related technological (energy conversion catalogues). Knowing these numbers, it becomes easier to set an optimal installation. 
-However, it is difficult to have a precise idea about digital energy consumption. Several false ideas need to be revised, such as saying that a computer system that is not used consumes compromized between cost of the system and performance. full power. Today, we also take into account environmental factors such as the impact on energy consumption when using full power. The idea is to federate the internet infrastructure sciences, which are not often energy and environmental sciences, to be used at saturation.
-It is difficult to find a way to put music on all servers with the same energy consumption.
-There are many environmental and social constraints; it will require different power profiles for different servers.
-The goal is to find an alternative method.
-Knowing that some servers don't have time (for example, 10 minutes for heat or 10 minutes for electricity) leads to inefficient use of power. 
-Optimizing this consumption requires modeling every possible scenario (dependent on the actor and the industrial sector). The document discusses the excessive size of a problem and how to reduce it by creating public cloud models as well as private models for specific needs (type clusters). 
- It is important to model physical systems using distributed computing architectures in order to account for various user needs. 
- The theoretical calculations are wrong when adjusting the physical precision chosen. This is because of the increasing number of physical layers, which will lead to an increased computational cost. 
- The goal is to understand system behavior (for example, degradation in energy consumption and system performance), but more time resolution is needed in order to design practical systems. 
- In addition, appropriate information must be available for the user's need. The document discusses the various constraints that gas must meet in order to be viable. This includes a need for natural gas in a transitional phase between many interdisciplinary and exchange opportunities. 
-Carbon-free hydrogen systems; low-carbon power sources; intermittent renewable energy sources such as solar and wind will all be constrained by the availability of a renewably sourced, low-carbon fuel in terms of peak power demand or industrial processes, or even transportation routes and food supply. 
-Heavy rail infrastructure will also be constrained by current fuels due to their high emissions levels. 
-The third axis focuses on improving energy efficiency of infrastructure. Knowing that new generations of mobile devices are piling up, we need to compensate their consumption with ours through sectoral integration (ISE). 
• A multi-vectorial approach is needed to ensure decarbonization while still meeting societal acceptance for energy sobriety. • Sectoral integration (ISE) is key to facilitating the penetration of renewable electricity into our systemwide energy mix. The European Commission has released a strategy called "Integration of Existing Infrastructure Systems" (IAIS), which aims to ensure the flexibility and coordinated operation of the system.
- The three pillars of IAIS are: local energy systems, "circular" energy systems with an emphasis on energy efficiency at the center, and increased interconnection and use of digital technologies.
- One of the main needs is for more efficient use of electricity, especially in sectors that can benefit from it (e.g. heating and cooling, industrial processes or data centers).
- Another need is for increased electrification in sectors that are suited to it (such as transportation). Cybersécurité is a major issue for digital. There are various groups of discussion on how to protect oneself from cyberattacks, such as the European Union's Cyber Security Action Plan.
-There are many vulnerabilities that need to be addressed, including those related to energy systems. In order to ensure a smooth operation of the entire system, it is important that all actors/operators in the system work together cooperatively.
-The 10-15% of an organization's budget allocated to information technology provides a global approach to risk management, and cyber security is essential in this context.
-The increased spending on cybersecurity measures comes with additional costs, especially for industrial cybersecurity. This could lead to increased energy prices, but it is essential in order to protect data integrity and achieve an equivalent value; continuity of function of the system as a whole. 
-Collaborations between sectors are necessary in order to manage the cyberthreat effectively. The energy system is made up of different systems that coexist, communication between sectors is necessary. 
- It is important to have a approach to control and optimize the interdependencies. 
- A harmonized regulatory framework in the EU; the NIS 2.0 are steps in the right direction. 
- Cybersecurity risks associated with an intense digital network and real-time are necessary to be managed. 
- A system for cyber certification for decentralized control (e.g., Edge-to-Cloud) will bring value but also new points of cyber vulnerability. 
- Today, due to the great industrial diversity not yet touched by cyber security, it is only too often regulatory transcripts in academic institutions. Member states, the EU seeks to put in place regulatory constraints towards industrial companies. An important investment in theoretical and practical training is necessary. The European Union's NIS 1.0 resilience directive reveals great differences in implementation between member states The SolarWinds attack has also revealed major vulnerabilities in the energy sector: the lack of preparation and the low 222304 ANALYSE SECTORIELLE 1/ ELECTRONICS, INFORMATION TECHNOLOGY, TELECOMMUNICATIONS

-Understanding digitalization begins with the physical components of a digital system. France has the potential to fill the value chains in digital transformation: 
-The French industry is mobilized on issues of electronic consumer objects (cycle life analysis). The proposed challenge is to reduce consumption by 1000 within 10 to 15 years. To achieve this, CEA-LETI proposes an action plan with 9 axes. Each axis is defined also on technology: 
1) Semi-conductors; 
2) Artificial intelligence; 
3) Data centers; 
4) Telecom companies Orange and SFR; 
5) Supply chains controlled by Atos and Kalray. The document discusses the levels of architecture and the last level of software embedded closely with the targeted application. 
-Today, in order to address the five-year problem, it is necessary to have an approach to calculation and energy consumption for systems on these three levels. 
-The first level, which is short-term, can be described as neuromorphic computation. STMicroelectronics is currently working on 28 nanometer chips; CEA-LETI is preparing the 10 nanometer cloud. The issue is to be able to mount intelligent objects on a market that will be available in 2025-2026. Under 10 nanometer circuits of intelligence embedded into a single nanometer scale, SOI interest may not be very clear. 
-A related object linked with IoT technology. In terms of performance of calculations and energy efficiency, we are still far from bee intelligence’s capabilities, and much work needs to be done in this area. We can look for concepts of stacked sheets of nano materials that could lead us closer to bee intelligence’s capacities and modify architectures so that memory and calculation units are closer together, STMicroelectronics is very present on the market because energy consumption represents 90% of data consumption in IoT scenarios. Processor traffic on the internet will continue to grow "Today, we can use memory from the 5G, which will allow for multiple IoTs. Non-volatile, coherent with the technology information exchange could reach 25 CMOS. Several comparison criteria are at 50 ZB in 2025 (1 ZB = 1021 octets), and 500 ZB defined, as programming power, by 2030." 
- "Intelligence artificial is used by IoTs such as endurance, writing speed etc... On explores a se realises in the cloud or data centers, via resistive memory and an 0-electric memory which running on large calculators. They offer significant relevant compared to ST Microelectronics estimates that flash data centers using a lot of memory. We can already consume 25 to 50 times more have a very low consumption." 
- "Short term, there are other means to reduce consumption. But long term developing a silicon substrate type insulator efficient, is to maximise local treatments (SOI) and using a technology called edge computing microcontrollers; it offers intrinsically an interesting alternative in terms of energy consumption, especially for embedded applications." Edge computing is a way of doing computing that is closer to the data. 
-It allows for faster calculations because it uses local resources. 
-There are two main players in edge computing: Intel and Nvidia. 
-Kalray focuses on accelerating artificial intelligence (AI). 
-Edge computing will reach 80% by 2030. Edge computing is important for tasks such as autonomous driving, which require fast processing of data. Commande requiert beaucoup de communication avec des informations locales (fréquence ultra-basse). 
-Les microcontrôleurs doivent être dotés de systèmes de communication intégrés et de moyens de sécurité embarqués. 
-Kalray construit des processeurs manycores basés un traitement partiellement analogique et non sur des cores programmables non spécialisés. Graphiques are capable of handling EPI (European Processor Initiative). The second information more quickly and with less steps will be to target more complex calculations, consuming than traditional CPU treatment gpgpu then on modern applications.
- Directly in memory. It will involve giving an acceleration feasible for local treatments require software applications of the second step.
- Kalray should be integrated into microprocessors, and then beyond current limits of technology want to offer an alternative to gpgpu and pass their associated neuronal networks, and finally beyond the limits of current technology.
- The ultimate goal is a self-powered system. The document discusses the negative effects of PPAs (Power Purchase Agreement), climate action). One way to guarantee an electricity supply is by using renewable sources (with, in return, a contribution to another report), but the European Commission warns that this market could weigh up to 6% of GDP during the decade (with, mechanically, a need for significant investments in spite of these different levers). That is, the debate is complex, with data centers putting strong pressure on various systems and the deployment of these equipment (and electric and energy consumption growth) being seen as additional expenses in infrastructure (in both sectors like telecommunications or transportation transformation, for example in Europe's economy as a whole). 
-Various projections attest to this problem on a global scale as well: by 2030, data center emissions could account for around 30% of Denmark's electricity consumption and Ireland's national electrical networks. 
-An illustration can be made comparing electronic lighting from early lamps that were very aggressive in trying to attract users to switchover. In 2019, France announced a moratorium on new light fixtures. très médiocres, puis on a vu arriver les data centers. 
- lampes à incandescence avec des rendements de 1 à 2 % , puis enfin les leds avec un rendement d'Il est essentiel que ce débat progresse, compte tenu lumineux allant jusqu'à 25%. 
- Comparativement, des data centers en 2030. 
- On observe avec intérêt les data centers sont à un niveau encore plus bas qu’une large coalition d’acteurs se mobilise autour que la lampe à huile et il faut espérer que, dans les dix ans qui viennent, le même progrès soit réalisé pour atteindre cet objectif. 
- Le constat de base n’est pas nécessairement alarmant. Pour la décennie 2010, on a remarqué une stabilisation de la consommation énergétique des data centers. Ce constat est expliqué par le développement de data centers « hyperscales » (i.e. « géants »), nettement plus efficaces que les précédentes générations The document is a list of signatories, including mega-flops. The Atos Sequana supercomputer, the old data centers, with 44 million giga flops for zones warm by 1.4)
-A supercomputer is a large radiator. 
-2 . The goal is to have 100% of energy used be carbon-free by 2030; this includes generating heat up to 200 tonnes. 
-Each component has freedom to choose its own energy mix. And already produces extreme heat, reaching 300 watts at 75% in 2025 (for operators today and 800 watts within 2 years). It can have a power greater than 50 kW. 
-10 million hearts can be installed to reach one of the green computing paths currently being pursued: tying consumption to today’s powers of calculation up to 400 green medium production methods and creating a petaflops (1 petaflop = 1015 operations per second). 
-The relationship between consumers and producers is deteriorating as efficiency decreases with increasing energy usage. This is what OVH does for servers it installs. The problem isn’t number of nodes, we’re not progressing anymore. 
-Specifically French, and OVH also works with Italy, India, North America on this issue. Electricity access may be put in place under contract agreements specifically for supercomputers - Power Purchase Agreement (PPA). This becomes more difficult when guaranteeing an electricity supply elsewhere becomes compulsory and renewable (with Atos has a quantum computing platform called Quantum Learning. 
-The platform is used to model the noise of physical devices and to remove noise for supercomputers. 
-There is also work on weather predictions, climate modeling, and quantum algorithms that are directly linked to prospecting for oil and gas. The document discusses genetic health, quantum computing, and the potential risks to drug design.
-Beyond just resisting future quantum attacks, there is also a need to acquire knowledge about quantum mechanics in order to learn how to optimize algorithms for autonomous vehicles.
-The goal of the European Processor Initiative is to give Europe the capability to create its own processors. By 2030, it aims to have reduced emissions by between 65% and 70%.
-More than 50% of private sector purchases involve processors that are available today. And within the telecoms sector, power efficiency is a key focus. The purchase agreements for the new data centers are in 2023. They will include elements of security and aspects of the digital world.
-The analysts expect that the training process will be effectively handled, taking into account these investments for artificial intelligence; however, they do not take into account the impact on carbon emissions or energy efficiency.
-Energy demand is much higher than projections, unless massive assistance is given. All of the sector is moving towards being carbon-neutral. 
-Today, Orange's data centers represent 9% of telecommunications sector consumption total. Orange consumes nearly 5.3 trillion kWh, 2 billion kWh of which comes from France. 85% of this consumption corresponds to network and information processing activities. Data centers represent 9% of the telecommunications sector’s total consumption. 
-Year after year, Orange accumulates carbon pits. It also plans to buy from specialized actors. There are two ways to buy: as produced (including intermittent service), or through an actor, who can balance with other sources. This resolves the risk of intermittent service, but is more expensive. We are trying to shift users towards fixed systems. We need to tie usage and architecture to consumption. The important thing is to animate a global ecosystem. 
-In terms of hydraulic engineering, we want to increase performance economic and industrial efficiency, improve project management and develop new capacities. In the electric power field, digital transformation is already a permanent fixture in terms of supervision and maintenance functions being centralized at the level of electricity generation, and having a strong role in business flexibility . 
-For EDF, the objectives for digitalization are to improve process management within the company as a whole; innovate constantly; create new services and lines in real time; optimize continually; and provide employees with better skillsets . 
-The VPPs are essential for achieving these goals The document discusses the benefits of virtual power plants, which allow for the development of skills and expertise, and the deployment of a robust technological infrastructure through virtual reality.
-The blockchain is used to trace the origin of data around energy, with the aim of developing systems that are ethical, cyber secure, and able to handle intermittent production.
-In terms of Europe-wide impact, taking into account new uses and decentralization of production, digital technology has made a significant contribution so far. 
-First and foremost, digital technology has helped speed up industrial processes (supervision, maintenance, optimization of network performance), but it has also created an opportunity for EDF to become a major player in distributing electricity across an extended customer base; this was crucial during the crisis.
-Digital technology’s main contribution lies in Industry (supervision, maintenance, IoTs 5G), but it is also important in creating a platform for distribution that can meet consumer needs in a more efficient way. The digital world affects the necessary functions for data reference production: production and energy consumption. 
-Security of participants can be improved by deploying 4G secured in fact become the operator of data in central power plants), deployment of IoTs to serve the carbon reduction of the entire (listening, visualization), providing assistance to industry, personnel, and public sector certification of test and production at an overall .of control...Moreover, edge computing goes in the direction of responsibility, bringing a capacity for production and functioning locally. It shortens decision times and discharges network congestion. Regarding nuclear power, the objective is to increase reactor lifetime safety, build tomorrow’s power plants, and dismantle old nuclear sites. Regarding new boiling water reactors (BWRs), we use twin digital systems in Section 302-3 / COMMERCIALISATION ET SERVICES The phenomena that system wants TO avoid are: • frequency collapse when one has ENERGY primarily electrical phenomena: • price imposition on energy market; then followed by distance between production and providing services accompanying consumption; ecological transition • voltage overload cascade. The climatic conditions necessary to manage energy consumption and the distribution of electric current depend on the efficiency of energy production, and finally on geographic distribution of energy management equipment around production and consumers. It depends on data provided by artificial intelligence or developing methods to account for the impedance characteristics of network elements, related to digital twins or connected objects. And finally, we need to develop strategies for reducing network topology, which represents its energy consumption intelligence. Even in a closed system like this one, the network manager ensures that the flow remains stable under normal conditions and can be degraded by losses. All this is accompanied by strong requirements: • a reliable service: with the example of an energy project. The simulation of a loss in a cloud-based system meets European level requirements through Gaia-X’s policy of anticipating system responses. It is undeniable that ENRs are increasing significantly the variability and uncertainty in electric flows. We seek to establish a transparent exchange with all actors involved, but also apply privacy by design principles in RGPP (Regulation for Electricity in Full Transition), which is undergoing a revolution. The means of production ENRs are rapidly becoming digital: they are poorly controllable because of their strong The uncertainties about the predictions for either IT for green or the green-IT, sunlight and wind. In this context, we are looking to minimize storage of RTE needs to manage and adapt the network in time, with an "ethical" approach.
-Digital technology allows implementation of using AI . various measures. Thus, RTE can make intermittent cuts in ENRs for a period of 2-4 / LE TRANSPORT D’ÉLECTRICITÉ of around a few hours per year. This allows better network management: RTE, the French manager of the transport network reducing 0.3 % of global ENR volume, estimates electricity between 400 kV and 63 kV manages the network by 2025 to achieve an economic saving of €7 billion compared to traditional investments, connections with industrial companies, euros on infrastructure investments. In addition, connections with the largest producers of ENR. The transport electric power behaves like an infrastructure with three pillars: voltage, current on each line and global frequency of the network. 
-Another solution is to change the architecture of power plants, maximum capacities of batteries , piloting system electric grid . Up until now , all these elements have been managed using two layers: are parameters for optimization made by the electrical post (local protection , time simulation model). We apply a prediction model - in seconds) then central level (on a control , whose principle is to control continuously over 10 min). The mass of information becomes closed . On analyse l'état du réseau en fonction des difficultés à gérer en temps réel ; on propose donc différentes actions. 
- L'installation d'une couche intermédiaire, qui sera pilotée par un navigateur, permettra de résoudre les problèmes de congestion typiques sur les réseaux électriques. 
- Les leviers à disposition des automates sont la co-simulation avec un simulateur de réseau et l'adaptation aux zones. Automates that can: 
- Change network topology; 
- Show the potential of cloud to edge systems by opening switches for auto-adaptive cloud to edge systems; 
- Be free with maximum benefits: economy of bandwidth, but increased risk of general outage; 
- Pass latency, robustness against production losses, and enable flexible solutions through smartwire batteries. This option has a high investment cost at the outset, but is cheaper to use over time. With these levers, the machine incorporates its actions into a model of the electric network that allows it to perform simulations. It provides costs for its levers and constraints on areas (like power limits), The document discusses the pros and cons of automating transportation.
-The document states that if we were to automate transportation in the same way as it is done today, with a fleet divided by 10, a number of vehicles that travel divided by 3, and a number of vehicles parked divided by 15, then Transportation would be reduced. 
-However, because of the prospect of autonomous vehicles, consumption for cars has decreased by 6.5% over the past decade. For a longer period of 10 years, as shown in the Pichereau report (1990-2010), corresponding figures are - 17%. 
-The problem is to be able to guarantee an absence on average consumption. Diésélisation leads to accidents. As technically driverless driving has provided an additional improvement of 15%, it must have recourse to artificial intelligence in order to have experience with situations before and after introducing direct injection into engines. Hundreds of piloted (1991), there is a gain of 26% in mileage recorded with recording equipment when compared to non-equipped cars. This was made possible by improved piloting with equipment for location detection and very fine levels of variable driverless control. To build a more complex system and pressure stronger at the level of learning so as avoid accidents, from engine power. It must control and optimize a large number of variables. One is consumption data The scenes of driving recorded, the others emissions of pollutants (ozone or made from “white leaves”. These and particles). simulations also test reactions system to gasoline, with electronic injection starting rare events. well before. In 1984, already 2Ko memory was used with a gain of 15%. Then direct injection began, with a gain 
additional 10%. Another part is that automatic transmissions quickly take decisions driving consumes 20 to 25% less than models they have access to. They are refreshed periodically and only powerful computers can provide them. 
The last step in digitization concerns very powerful processors that can combine the thermal engine, American, Tesla and Waymo are developing their own processors and call it hybrid motorization their computers. Waymo plans a car with 5 lidars (a method for automobile navigation. This has not prevented transport by telemetry and detection similar to radar), 27 cameras, 6 radars and an enormous power emission of CO in Europe.
2 The strenghening of emissions standards for new vehicles will require a significant investment by automakers croissante des moteurs électriques et hybrides rechargeables ;
- impact direct sur les véhicules thermiques ;
- part transformation, c’est l’assistance à la conduite ;
- au de marché devrait atteindre 40 % en 2030 ;
- niveau extrême du véhicule autonome auto-partagé ne prévoit guère de changement au on pourrait réduire considérablement le nombre de km parcourus. The development of a safe artificial intelligence, the cost of conception, and its lack of noise are all significant. But the needs for calculation are huge in terms of cybersecurity and securing large exchanges; Airbus uses powerful computers among data from automated vehicles. It is important to remember that this has a positive environmental impact by allowing vehicles to go further with less fuel. 
- The increasing number of electric motors in aircraft controllers requires more finely tuned control systems. Data analysis allows Airbus to offer predictive maintenance services that are increasingly important in its business. This is the platform Skyways used by many air carriers. Airbus’s power is sufficient so that it was able to keep control over this platform, which it developed with Amazon. The impact that digital technologies have on reducing aviation emissions is not insignificant, especially when optimizing flight trajectories: one key way to reduce aviation emissions is to use finely modeled meteorological data for maximum benefit from wind speeds. This optimization requires great computing powers. The document discusses the goals of the French government for digital, energy, and infrastructure services. Three objectives are pursued: a technological objective, touching interconnection of the network; an economic objective, to obtain a viable model for smart grids; and a regulatory objective, to bring all participants in construction together so they can understand how design changes should be made.
-A digital model for energy consumption management has been developed. This model allows planners to visualize behavior of buildings over time and manage them more efficiently based on their consumption patterns. 
-A twin digital model has been created for testing different architectural configurations in an urban setting. The first phase was delivered in 2015 and showed a decrease (with the aim of better designing according to real-time data) in energy consumption by 60%. The second climate and solar contribution, but also step was at neighborhood scale; geothermal has constraints of dimensioning as well as seismic use.
-Furthermore, with still decreasing. . The third example, in BIM, of technical constraints, legal constraints, from a quarter of Lyon, allowed for realization of a training course to be taken into account; urban service platform at the level of an island. • a service urban platform guaranteed in time by a management of buildings gives an opening contract urban contract. to new urban operators. With communication meters it is possible to manage and make distant erase for electricity production and distribution at building scale (cabling etc.) ; • the last aspect is to integrate the building into the neighborhood scale. The smart grid has changed its scale and is moving towards the neighborhood. Experiences show that an analysis and optimization work is necessary. In 2021, one building was able to be piloted with an tool that retrieves real-time energy consumption and production on a daily basis. In practice, there are three levels to deal with: network infrastructure, information system management and city design conception. The third demonstration on a surface near 300,000 square meters;
- The project was financed with private funds, with a very low budget for the price of a demonstrator, showing that it is possible to carry out important operations with a reasonable budget. For Issygrid, although the oldest buildings date back only 10 years, the global information gathering (electric network, residential area, commercial) has encountered some difficulties. For example, there were 14 interconnected systems that were not all standardized by the same protocol. This difficulty was overcome despite technical problems and population mistrust of energy monitoring (about 50% of residents are using it). First logical erase schemes could be tested thanks to piloting with standardization of the digital system. Regarding energy profile, renewable heating reached 85% of needs compared to 50% in Paris In order to increase performance, sensors were added with the help of network operators: CPCU (urban network), water from Paris (deep geothermal), Enedis (electric network) and social landlords. With this work, information retrieval became possible even at individual level (with consent contracts), 
- even though globalization does not work perfectly yet. It was also found that self-consumption works well.
- Additionally, a model neighborhood quarter was built to understand annual and monthly consumption levels and it was shown that geothermal works well too; however, globalization does not function as perfectly as planned yet. It was also found that self-consumption does not work as well in this case Collective est more efficient than individual. This requires discipline to use these modes at specific times and a tracing of residents (through blockchain technology already in place). 
- Three important topics summarize the challenges and difficulties faced by the new digital ecosystem for the neighborhood: computer standardization, twin digital technologies, and flexibility. 
- Even though electronic-informatics sectors benefit greatly from the digital age, not all applications should rely on it: 
- Energy efficiency benefits both users and producers: 
- The main positive consequences are that users save energy: 
- Efficiency of uses is key to mitigating climate change. However, increasing demand for computing power and quantum technology will require more resources in the short term. The document discusses the impact of digital technology on energy consumption.
-Manufacturing has become increasingly reliant on digital technology, which has led to increased energy consumption.
-The consequences of this are that manufacturers must find ways to reduce their energy consumption, and recycle used electronic equipment.
-This is a radical transformation that will require companies to change their practices in order to conserve resources and reduce emissions. The document discusses the European concerns for energy and the need for sustainable progress. 
-One of the most important aspects is the challenge to programmers in terms of skills, location, and electronic technology. 
-The goal is to reduce costs while still maintaining high performance. 
-Europe has been behind in terms of developing systems, but this is now changing with advances in technology. 
-Separation between software and hardware will not be possible forever due to advances in miniaturization. Samsung is leading the Asian automotive industry in terms of engine performance and comfort.
-Intel had seemed to be giving up, but has decided to recommit itself. This has continued to develop. But we need to take steps to get back on track, and adopt a more aggressive policy that will require open communication with customers: increased use of artificial intelligence (AI) and digital technologies, based on closer ties with suppliers and joint development of manufacturing facilities in Europe.
-In terms of the local network for the car's digital ecosystem: 
-The car's processors (parallel processors, servers, autonomous driving), applications and uses will all change profoundly over the next few years. 
-France is currently the only European country that can claim this level of global leadership in integrated circuits and sensors technology.
-EuroHPC is designed to maintain Europe's leading position in circuit integration technology and sensor arrays while expanding its reach into data processing at the local edge (edge computing). This will encompass consideration of energy consumption within its overall goal of reducing reliance on imported products by 30% by 2030. The changes made by European policymakers on all types of equipment; this objective will be taken into account when assessing energy consumption as well. Pays demander des compétences nouvelles. 
-Actif sur la totalité de la chaîne, de ST à Atos en passant par Kalray et OVH. 
-Cela lui donne AÉRONAUTIQUE la responsabilité d’une prise de leadership dans la construction d’une position commune. 
-La simulation va jusqu’à faire « voler au sol » les En télécommunications, l’événement actuel est le avions que l’on conçoit. 
-Les gains en temps et passage à la 5G, qui va ouvrir l’Internet des objets. coût de conception, et en sobriété des avions, Même si elle est énergétiquement plus efficace, il va sont considérables. 
-Mais les besoins de calcul sont falloir assumer un accroissement considérable du énormes, et Airbus utilise des ordinateurs parmi les trafic sans que cela se répercute sur les émissions plus puissants. The document discusses the problems of touch affecting technology, standardization, and data ownership. One of the major operators and their relationships with the energy issue is to conceive a government-wide energy grid, market rules, and neighborhood demand: reducing research by 60% could be achieved. And self-consumption in research could be much more effective than individual consumption.

-EDF wants to be the operator of the energy system, but must take care to avoid monopolizing data, which are owned by the large technology companies. There were thirty years ago when electronic devices started being introduced into cars, for the purpose of improving driving safety. 

-In order to address these gaps in analysis, it will require many years for proper studies to be conducted. In digital terms, with new regulation coming into effect, this will require remedying it. It requires material-logical interaction; now that is standardizing what are considered points of measurement for a majority of our systems. 

RECOMMENDATIONS: 
1) To account for Europe's comparative advantages and make them known, it is necessary to build solidarity among its citizens through an understanding of how their individual actions impact society as a whole. 
2) It is important to take into account all factors contributing to digital consumption and use our advantages in manufacturing and operating systems: achieving dominance or at least autonomy is our goal. France has advantages, such as the global measure. France needs to be present in all aspects of the digital chain for example in the field of computer science. 
- There are not enough explorations between algorithms and their relations with each other, cyber-security being an important area. 
- To manage the increasing diversity of algorithms, a global understanding is needed. This knowledge will allow specialized operators to emerge. 
- The future path is that of specialised operators, so close interaction between hardware manufacturers and component suppliers is necessary. France should take advantage of this new technological era. 
- Invest in research and development on new information technology technologies; 
- Develop programmers who are well-equipped to work closely with designers of hardware and software, and able to maintain the digital heritage, also ensuring cybersecurity. 
- Redesign the application software's digital heritage to benefit from the contributions of specialized components and massive parallelism; 
- Pursue brain networks, neuromorphic computing, and artificial intelligence (AI), while not neglecting socio-economic support.