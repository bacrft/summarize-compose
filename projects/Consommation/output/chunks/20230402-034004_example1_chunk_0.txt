Les auteurs présents sont : 
- Jean-Christophe Air Liquide (président), Marie-Cecile Ancre/INP Grenoble, Dominique France Stratégie, Philippe CEA, Azarmahd Telman EdF, Seddik INP Grenoble, Bareux Gabriel RTE, Bertière François Académie des Technologies, Burttin Alain EDF, Busato Guillaume RTE, Chailot Christophe Orange, Clausse Marc Ancre-Allistene/INSA Lyon, de Warren Nicolas Uniden Dupont de Dinchein Benoit Kalray FAUCHEUX Ivan CRE FERRASSE Jean-Henry Aix-Marseille Université Ferreboeuf Hugues The Shift Project Fournel Guillaume CRE Game David RTE Gelinbe Erol Academie des Technologies Giraud Guillaume Engie Giraud Guillaume La consommation d’énergie du numérique est en augmentation
- Les limites physiques et technologiques sont connues
- Les infrastructures numériques ont été aménagées selon les besoins actuels
- La sécurité et la résilience des systèmes informatiques sont importantes Le groupe a réuni entreprises, organismes de mais les prophètes de malheur exagèrent : ils ne recherchent pas et pouvoirs publics.
- Ce groupe est tiennent pas compte des progrès du numérique, réuni depuis 2017 par l’ANRT, sous la présidence y compris dans la manière de s’en servir.
- Le problème demeure, mais nos travaux montrent de quoi il l’est moins dans les secteurs applicatifs que à l’analyse des enjeux technologiques liés dans celui du numérique lui-même. Les technologies renouvelables intermittentes constituent une option pour réduire la consommation électrique.
- Les énergies renouvelables intermittentes ont des avantages sur les énergies conventionnelles, notamment en termes de climat.
- La diminution de la consommation électrique est un point important du cadre stratégique du COP21, mais il est aussi nécessaire d'analyser l'empreinte numérique des technologies. Le numérique est une révolution qui changera les chaînes de valeur.
-Le problème est que si nous ne réagissons pas, les pays qui maîtriseront cette révolution seront ceux qui maîtriseront l’interaction entre l’électronique et les produits.
-La France et l’Europe font face à un double problème : il faut maîtriser la chaîne du numérique et que les secteurs applicatifs disposent du personnel compétent pour concevoir le numérique utilisé dans la conception, le fonctionnement et la maintenance des matériels qu’ils produisent. Les technologies s’est beaucoup améliorée. Cela a démenti les prévisions catastrophistes, comme celle parue dans Forbes en 1999, selon laquelle en 2010 la moitié de la production américaine d’électricité serait absorbée par l’économie numérique et Internet.
- Les prévisions sont difficiles, en raison de la rapidité avec laquelle la demande, l’efficacité des TIC, et la manière dont on les utilise évoluent. Les méthodologies et les hypothèses diffèrent selon les organisations. Les résultats extrêmes, sur lesquels on communique beaucoup, résultent de modèles simplistes. Par exemple, pour la consommation des centres de données (data centers), ces modèles donnent des résultats plus de deux fois supérieurs à la fourchette probable (200-350 TWh).
- Pour contenir la consommation et les émissions, il faudra investir de façon considérable dans la recherche et le développement. Et s'il faut réduire autant que possible leur empreinte directe, les plus grands impacts des technologies numériques sur les émissions proviendront de leurs applications dans d'autres secteurs (electricité, transport, bâtiment,...), bien que leur emploi puisse en fin de compte réduire sa consommation et ses émissions totales Les hypothèses du document prévoient une baisse globale de la consommation électrique du numérique, mais celle-ci est difficile à quantifier 
- La consommation des data centers et des réseaux augmente sensiblement, tandis que celle des appareils mobiles diminue 
- La fabrication des équipements représente aujourd’hui 40% de l’impact global du numérique L’empreinte carbone est la quantité de gaz à effet de serre émise par un produit ou une activité. 
- Les secteurs ICT et E&M représentent ensemble plus de 60 % du PIB mondial. 
- La durabilité des équipements réduit l’impact de leur fabrication. 
- Des réglementations et normes existent, notamment sur l’obsolescence programmée. Le trafic internet va continuer à augmenter de façon importante 
- La consommation énergétique du numérique est difficile à mesurer car il y a une méthodologie évolutive et par manque de données fiables et exhaustives 
- La fabrication des équipements représenterait aujourd’hui 40% de l’impact global du numérique 
- Les projections des gains de productivité sont très divergentes Le processeur est la machine thermodynamique qui réalise cette acquisition : pour accroître la connaissance d’un système donné ou de manière équivalente baisser son entropie, elle consomme de l’énergie noble (d’origine électrique), et la dissipe dans un thermostat en conservant globalement les transferts d’énergie, selon le premier principe de la thermodynamique.
- Sa machine duale étant le moteur de Szillard (1929), la classification thermodynamique suggère de voir le processeur comme une machine frigorifique dont l’efficacité est définie par un coefficient de performance. Ce dernier est utile pour calculer le service thermodynamique rendu par la digitalisation au terme d’un cycle élémentaire de fonctionnement.
- Afin de disposer du processeur pour le calcul élémentaire suivant, son cycle de fonctionnement se « ferme » sur un mécanisme d’effacement (paradigme de Landauer3) qui permet de rapporter la valeur de l’information acquise à l’énergie qui lui a été allouée par la polarisation des circuits d’électronique logique. La loi de Moore décrit le nombre de transistors sur une puce de silicium double tous les deux ans. 
- La loi de Koomey décrit le nombre de calculs par Joule dépensé chaque année. Koomey est une extrapolation des lois de Moore, qui prévoit que les ordinateurs auront atteint l’extrapolation à la fin de 2020.
- Les limites de la loi de Moore sont décrites ci-après : les premiers signes d'essoufflement apparaissent depuis le milieu de la décennie 2000 - 2010.
- Afin de repousser ces limites, d'autres paradigmes de calcul ont été proposés, notamment en 1973 par C. Bennett d'IBM, qui s'appuient sur une distinction entre entropie physique et entropie computationnelle : l'entropie physique est associée aux fluctuations d'un système macroscopique pour atteindre l'équiprobabilité de ses micro-états ; la chaleur dégagée est liée aux variations de ces fluctuations quand le système évolue et l’irréversibilité est due à une évolution imposée en un temps fini ; l'entropie informationnelle est associée à la perte d'information ; la moitié des pertes est due à l’irréversibilité (effacement de la mémoire des entrées), l’autre moitié à la dissipation des circuits de commutation des transistors polarisés par des échelons de tension. Le ralentissement de l’ordinateur quantique est due à la capacité des microprocesseurs à dissipater une énergie considérable.
-Les limites imposées par la puissance dissipée et la densité d’intégration valable tant qu’on ne mesure rien compliquent de plus en plus les progrès possibles en matière de miniaturisation des transistors. Les transistors quantiques sont constitués de deux conducteurs minces qui se trouvent à une distance physique limitée les uns des autres. 
- Les qbits sont capables d'être « cohérents », ce qui signifie qu'ils ont un état qui correspond à la valeur propre du vecteur. Le quantique est une branche du monde scientifique qui étudie les lois de la physique quantique.
-Le calcul quantique est énergivore parce qu’il requiert beaucoup d’énergie pour faire des opérations sur les états de base.
-Les « portes » quantiques sont réversibles et ne transfèrent que très peu d’énergie. Oracle permet de changer le signe du coefficient ce jour. 
-De l’abonné qui a le numéro de téléphone donné au départ. Il reste alors à appliquer une série de Le calcul quantique permettrait théoriquement de : 
-factoriser de très grands nombres (cryptogra- phie : algorithme de Shor) ; d’être inversé (suite de l’algorithme de Grover). 
-trouver la configuration d’ énergie minimum L’état du système est alors “presque à coup sûr” d’une molécule complexe en chimie ou l’état recherché. Pour avoir une certitude, il convient en biologie ; de refaire une seconde fois la même opération • optimiser des systèmes très complexes : ou de la faire en parallèle sur deux ordinateurs logistique, finance, réseaux de toutes sortes, quantiques identiques. physique des particules, ... ; • accélérer le fonctionnement de l’intelligence En pratique, un qbit peut être un électron isolé artificielle. avec un spin ou un noyau de spin 1⁄2, une jonction Josephson, des ions ou des atomes 10 bits 
- Beyond 8 bits, any interaction between this and the outside world is not for tomorrow. Quantum computers and the outside world can be considered as measures and create a separation that leads to errors. In reality, for most specialists, it would at least take 100 bits to make quantum computing interesting. The ways to reduce errors would be to work at very low temperatures and if possible with the minimum number of connections between inside and outside of the cryostat, or to calculate faster than thermal noise, or finally to correct errors like in traditional computers. 1503 INFRASTRUCTURES NUMÉRIQUES. STATE OF THE ART AND EVOLUTION 1/ PARAMETERS OF INFORMATION SYSTEMS THE MAIN PARAMETERS OF USE FOR INFORMATION SYSTEMS ARE: 1. Processing power expressed in number of operations per second. It itself depends on: • frequency; historically, we have been able to increase it constantly by pushing miniaturization of integrated circuits (Moore’s law). But now we reach 4-5 GHz; • number of integrated circuits per processor, and number of processors; • type of calculation, with today’s introduction of special processors for increasing processing power without increasing frequency (whose implementation results in parallel treatments). However, specialization of processors forces design simultaneously of circuits and software while universal processors could be manufactured separately, in very large series. Moreover, specialization diminishes up to 90% of existing software. 2. Intensity The latency, or time it takes for a message to be transmitted from one point to another, is related to the calculation power and system organization.
-The compromise between centralization and decentralization is reflected in architectural choices such as étagement and network deployment.
-There are two types of architectures: those with hierarchical network topologies and those with dispersed nodes.
-The European Union has set standards for 5G networks, based on the edge architecture. La complexité algorithmique dépend de la quantité de données à traiter.
- La plateformisation permet aux usagers d’accéder à des puissances de traitement qu’ils ne pourraient pas se payer.
- Mais la plateformisation a un envers : la propriété des données, ou au moins les droits d’accès. Le FOG computing est une architecture qui permet de spécialiser le support des applications et services IoT, en particulier pour les véhicules connectés. 
-Le défi est alors de mettre en place des outils de mesure permettant de comprendre la consommation énergétique du computing (des millions de processeurs avec des systèmes entiers). Edge computing est un ensemble de technologies qui permet aux organisations de réaliser des tâches complexes et variées à partir d’un petit nombre de serveurs situés sur les côtés du réseau, en évitant ainsi la congestion du système. 
-Les FOG sont des entreprises spécialisées dans le traitement et la prétraitance de données. 
-Le FOG est différent selon qu’on cherche la sécurité ou la rapidité. Le processeur universel est en train de disparaître.
-Le recours à la spécialisation et au parallélisme est le futur du processeur.
-Le processeur universel a été miniaturisé et il a pu être augmenté sa performance tout en diminuant sa consommation d’énergie, mais ensuite il a fallu arrêter l’augmentation de fréquence car la chaleur à dissiper devenait trop importante. Pour continuer à avancer, on doit recourir à la spécialisation et au parallélisme. Le document souligne les avantages et inconvénients du parallélisme pour les applications. 
- Les programmes actuels ne sont pas faciles à programmer et doivent être réécrits pour tirer parti du parallélisme. De l'apprentissage, soit des modèles dits hybrides qui sont des modèles physiques plus simples couplé à des modèles Les chiffres que l’on a actuellement sur la consommation énergétique. technologiques (catalogue de convertisseurs d’énergie, catalogue de moyens de stockage à consommation générale du numérique sont court et long terme). Connaissant ces données, on détermine l’installation optimale, notamment que c’est vrai depuis les débuts du numérique. Les unités de production, les moyens de stockage, Malgré toutes les améliorations de l’efficacité énergétique, l’accroissement du trafic et du Il s’agit de ne pas surdimensionner et d’éviter les nombre d’équipements conflits entre systèmes, mais en même temps de le déploiement de l’internet des objets (IoTs). répondre à la demande. C’est un enjeu majeur pour Les entreprises en énergie. Plusieurs idées fausses doivent être revues, comme dire qu’un Pour se fixer un objectif, le critère classique est le empreinte consommation pleine. peu. Un serveur qui n’est pas utilisé continue de Aujourd'hui, on y ajoute la prise en compte de la performance La plupart des carbone sont utilisés pour produire de l’énergie.
- Les critères environnementaux et sociaux doivent être intégrés dans les méthodes de choix énergétiques pour servir les serveurs.
- Il est difficile de trouver un optimum raisonnable (proche des performances du CPU) dans un temps raisonnable d’utilisation du CPU. Le volume des données à traiter est excessivement grand
-Il faut réduire le nécessaire en créant des données types clouds publics et privés
-On doit prendre en compte la performance, la sécurité des données et l’efficacité en temps réel Le gaz est une énergie fossile et donc contrainte de contraintes en termes de transition entre beaucoup d’interdisciplinarité et d’échanges.
- Les charbon et solutions décarbonées ; l’hydrogène systèmes d’information seront contraints d’un point renouvelable et bas carbone dans des procédés de vue énergie, en termes de pic de puissance, ou industriels, voire dans les transports routiers et d’alimentation en énergie renouvelable intermittente.
- Le troisième axe concerne l’amélioration de biogaz etc.
- L’efficacité énergétique des infrastructures. Sachant que les nouvelles générations de mobiles s’empilent, Concrètement, les caractéristiques de l’intégration sectorielle (ISE) sont : gains sur les usages dans d’autres secteurs. • approche énergétique multi-vectorielle, pour garantir décarbonation, sécurité Le dernier axe concerne la sobriété pour réduire d’approvisionnement, résilience et maîtrise des la consommation énergétique. La Commission européenne envisage de mettre en œuvre un « système énergétique circulaire » afin de réduire les risques liés à la consommation d’énergie et aux déchets.
- Les trois piliers de cette intégration sont : une efficacité énergétique au centre, le développement des «communautés énergétiques» et l’interfaçage entre les différents secteurs d’activité.
- Pour ce faire, il est nécessaire de sensibiliser les utilisateurs aux risques et à l’importance du back-up. Les principaux constats et recommandations liés à la cybersécurité sont les suivants: le rythme des changements induit de nouvelles vulnérabilités dont il faut savoir se protéger, les dispositifs de sécurité sont les meilleures pratiques et standards, mais toujours en retard sur les menaces; le système énergétique n’est plus linéaire, mais passant par différents acteurs.
- Aujourd’hui, toujours en retard sur les menaces; le système énergétique n’est plus linéaire, mais passant par différents acteurs. Les dépenses supplémentaires certes des coûts supplémentaires, notamment liés à la cybersécurité industrielle pourraient atteindre une valeur égale; continuité de fonctionnement du système entier est essentielle. Il est indispensable de contrôler et d'optimiser les interdépendances entre les systèmes énergétiques, communicationnels et industriels ;
- La réglementation harmonisée de la cybersécurité est un pas important dans la bonne direction ;
- Les personnes qualifiées restent insuffisantes pour gérer correctement les risques cybernétiques. Un investissement important dans la formation théorique et pratique est nécessaire. Les composants électroniques sont principalement fabriqués par des entreprises françaises.
- Les industriels travaillent sur les enjeux de la consommation et de l’efficacité énergétique pour les objets électroniques.
- Le CEA-LETI a un plan d’action pour réduire la consommation d’un facteur 1000 d’ici 10 à 15 ans. Le CEA-LETI est chargé de la conception et du développement d’applications et systèmes liés à l’intelligence artificielle.
- Le plan d’action propose de rapprocher les unités de calcul et de mémoire, afin de réduire la consommation d’énergie.
- Les objets intelligents (IoTs) représentent une consommation importante d’énergie, et le trafic des processeurs sur internet va croître. Les mémoires de la 5G permettent de multiplier les IoTs.
- Les critères largement pertinents comparés à ceux des 4G sont la puissance de programmation, en 2030, et l’intelligence artificielle.
- La consommation d’énergie pourrait être multipliée par 50 à 300 Twh d’ici 2030 si on utilise les mémoires de la 5G. Edge computing est une approche qui consiste à travailler près des données pour réduire la latence.
-Kalray est un acteur important du secteur de l’edge computing.
-Le but du edge computing est de réduire la latence et d’agir au niveau de chaque décision. The need for local processing means that a lot of communication is needed (with information; low frequency).
-To equip microcontrollers with integrated communication systems, and means of security embedded, with multiple homogeneous cores (data centers), installing inference processors with a multiplicity of cores is necessary. It is neuronal in local networks (neuronal networks are easier to program, but do not learn and process information well at scale).
-One challenge is to increase the memory capacity so that operations can be performed on it. Today, energy consumption is mainly due to communication between processors and memory. However, we have observed that processor designs based on integrated circuits with many cores are not always specialized, the most efficient being multicores and manycores.
-Kalray builds processors based on manycore processors which are partially analog and not based on programmable non-specialized cores like just binary ones. These machines are used in particular for edge computing where they are very effective in neural inference, as well as in high-level programming languages such as Java or C++. Les data centers constituent des équipements qui constituent les data centers et sont électro-intensifs.
- Les objectifs de stabilisation des réalisations d’OVH doivent être dépassés sans doute : les gains d’efficacité ne suffiront pas.
- La consommation énergétique est la plus prononcée indépendamment de l’alimentation électrique, notamment pour les activités déployées (bonnes ou mauvaises). The European Commission (EC) has proposed a new renewable energy market report, which could weigh up to 6% of the GDP during the next decade.
-There are various levers to promote renewables, but their concentration in data centers poses a threat to local systems as well as the transformation of the European economy.
-Projections show that by 2030, data centers could account for 30% of electricity consumption in Denmark and Ireland, and 20% in France and Germany. The document discusses the various types of lighting used in data centers, and how each type has its own pros and cons.
-The main point of this document is to give a brief overview of the current state of data center lighting, and to highlight some key trends that are worth noting.
-Overall, the document notes that while there are some very middling results from traditional lamps and incandescent bulbs, LED technology is finally starting to show real promise in terms of energy efficiency. However, it's still early days for LED lighting in terms of overall performance levels.
-One important goal that Europe has set for itself with regards to climate change is achieving "neutrality" with regard to greenhouse gas emissions - which means reducing them as much as possible without compromising economic or social progress. This challenge will require significant progress over the next few decades in order to be achieved.
-One area where progress has been made recently is in developing " hyperscale" data centers - which are significantly more energy efficient than earlier generations of facilities. 27OVHCloud is one such company that has set ambitious goals for itself based on these five pillars: high global performance standards. The Atos Sequana is a supercomputer with 44 million giga flops. 
-The PUE for this machine is 1.4, meaning that it uses 44 million watts of energy. 
-This machine will produce extreme heat, up to 300 watts at peak. 
-This machine can reach 75% renewable energy usage by 2025. 
-OVH plans to install 10 million green computing nodes in France over the next two years. Le programme Quantique de Atos est développé pour étudier les calculs en mémoire vive et la suprématie du numérique sur les processus physiques.
-Le Quantum Safe Program est également utilisé pour travailler sur les prévisions météo, le climat, les algorithmes quantiques et toutes les modélisations supercalculateur. Les technologies de l’information et de la communication ont un impact significatif sur les émissions de carbone.
- L’objectif du programme européen European Processor Initiative est de donner à l’Europe la capacité de créer ses propres processeurs. Purchase agreements are in 2023. It will include elements of security and the impact of digitalization on training.
- analysts do not take into account these investments in artificial intelligence when assessing the carbon impact; energy consumption is one of its issues and will need to be taken into account in new data centers with a low environmental impact.
- Energy demand is much more conservative than projections, and the sector as a whole is moving towards final, decarbonization led by two Asian champions: Samsung and TSMC. Intel, which had seemed to give up, has decided to catch up again and lead a more open policy: more dialogue with its customers, without relying on subsidies. This combination can result in growth of consumption energy while reducing carbon emissions. Les data centers d’Orange représentent 9 % de la consommation totale du secteur des télécommunications.
-D’année en année, Orange a accumulé des puits de carbone. Il envisage aussi de s’approvisionner chez les acteurs spécialisés.
-Il y a deux façons d’acheter : tel que produit (y compris l’intermittence), et tel que chez l’acteur, qui peut équilibrer avec d’autres sources. Cela pallie le risque d’intermittence, mais coûte plus cher.
-On cherche à faire basculer les utilisateurs vers les systèmes fixes. Il faudra savoir lier l’usage et l’architecture à la consommation. L’important est d’animer de façon globale un écosystème entier. Virtual power plants allow for the development of skills, expertise, and a robust technological backbone.
-The blockchain allows tracing the origin of data, which is important in developing energy technologies such as IoTs (digitalization within the company).
-On the European level, we are trying to secure transactions with systems of artificial intelligence. One way to do this is by developing AI systems that reduce the time it takes to process and respond to problems related to digital storage.
-For distribution, EDF is looking for market dominance by adapting its network to meet new uses and decentralized production. The contribution of technology lies first in industrial areas (supervision, maintenance, optimization of the entire network), but also in creating a platform for widespread dissemination. Impact of digital on functions necessary for reference data production: production and energy consumption.
- Security of participants (4G secured become truly operator of data in power plants), deployment of IoTs to serve carbon dioxide reduction throughout the whole economy (listening, visualization), assistance to participants, industrial, personnel and public certification of tests and productions at all levels.
- Edge computing goes in the direction of responsibility, providing a capacity for production and functioning locally. It shortens decision times and eliminates network congestion.
- Regarding nuclear power, the objective is to increase the life span of nuclear plants safely, build future power plants, and dismantle old nuclear sites.
- On new pressurized water reactors (PWRs), we use digital twins in the 302-3 / COMMERCIALISATION ET SERVICES phenomena that system wants ENERGETIQUES primarily to avoid:  • frequency collapse when there is a  • imbalance between production and commercial performance; responsiveness in terms of offering (with prices imposed on energy market exploding), then followed by related to distance between production and providing accompanying services; ecological transition and climate change • overload cascade of electric lines. La répartition des flux de courant dépend de la l’efficacité énergétique, et enfin de développer des services de maîtrise de l’énergie (autour des données) aux caractéristiques des conducteurs.
- Les exigences pour la cybersécurité sont fortes : il faut établir un échange transparent avec les acteurs, mais aussi appliquer le principe privacy by design. RTE cherche à minimiser le stockage de RTE
-Le numérique permet la mise en œuvre de l’IA pour gérer le réseau
-Diverses mesures ont été prises pour réduire les consommations d’électricité
-L’architecture du réseau doit être adaptée aux contraintes "on cherche des méthodes et des moyens de adaptatifs sur des zones."
- "Les leviers à disposition des automates sont et étant donné les constantes de temps, de l’ordre en passant par la co-simulation, comme de la minute, on peut mettre en place des actions associer un simulateur de réseau électrique automatiques." Les automates peuvent changer la topologie du réseau, ce qui augmente le risque de coupure générale.
- Les automates peuvent aussi utiliser robustesse face aux pertes de postes.
- L’effacement de production d’EnR et permettre une plus grande flexibilité sur le réseau électrique sont les deux principaux leviers des automates. La numérisation a permis de réduire la temps de transports.
- La consommation moyenne d’une voiture par km partagés reste lointaine, probablement au-delà des 6,5 %.
- Pour une période plus longue de 10 ans, comme l’expose le rapport Pichereau. (1990-2010), les chiffres correspondants sont - 17% The introduction of electronic fuel injection in 1984 led to a number of rare events.
-The processors that control the driving of autonomous cars are called neuromorphic.
-This has led to an increase in performance by 10%.
-Autonomous vehicles equipped with lidars (a type of radar), cameras, and sensors have seen a 27% increase in number since 2017.
-However, this has not prevented the growth of emissions from autonomous vehicles. La croissance des moteurs électriques et hybrides rechargeables aura un impact direct sur les véhicules thermiques, qui perdront du terrain.
- Au détriment des véhicules thermiques, la part transformation des voitures devrait augmenter de manière significative.
- Le nombre total de kilomètres parcourus pourrait baisser de 30 à 40%.
- Les gains en matière de R&D liés aux véhicules auto-partagés doivent être renforcés. La mise au point d’une intelligence artificielle sûre coûte de conception et en sobriété des avions est considérable.
- Les besoins de calcul sont plus puissants que ceux pour les applications courantes.
- L’architecture électrique et électronique du véhicule a un impact environnemental positif, permettant d’aller plus loin dans la frugalité des aéronefs. 
- Les logiciels de reconnaissance de forme constituent une menace pour les systèmes autonomes car ils peuvent décoder les signaux envoyés par le véhicule. 
- La puissance des ordinateurs utilisés par Airbus permet à l’entreprise de garder le contrôle de Skyways, alors qu’elle l’a mise au point avec Amazon. L’objectif technologique est de créer des outils permettant d’interconnecter le réseau et d’effacer les données. 
- L’objectif économique est de trouver un modèle économique viable pour les smart grids. 
- Le but du BIM est de rassembler tous les intervenants concernés par une construction : conception, exécution, contrôle. 
- L’objectif réglementaire est de déterminer la conformité entre la conception et les règles en vigueur. Les différents acteurs numérique, énergétique et immobilier ont été rassemblés pour travailler ensemble sur les projets.
- Les défis relevés avec l’exemple de Batignolles sont : 
- On a rassemblé les différents acteurs numérique, énergétique et immobilier ; 
- Le terrain de jeu s’est élargi, avec une partie résidentielle. Le document est relatif à Issygrid, un projet d’installation de réseau électrique tertiaire sur une surface près de 300 000 m2 en France.
-Le projet a été financé entièrement par des fonds propres, avec un budget très bas pour le prix d’un démonstrateur.
-Le but du projet était d'améliorer la performance du réseau et de l'information globale sur ce dernier.
-Les différentes difficultés rencontrées ont été dépassées grâce aux efforts fournis par les différents acteurs impliqués (opérateurs de réseau, bailleurs sociaux). Une maquette numérique du quartier a également été construite. La collective est plus efficace que l’individuelle.
- Ceci requiert la discipline d’utiliser ces modalités à des heures précises et demande une traçabilité des habitants (grâce aux blockchains déjà mis en place).
- Les principales conséquences positives sont le fait des usagers : réduction de la consommation (voiture, avion, bâtiment) ; efficacité des usages : si le concept de voiture autonome se généralisait on économiserait 30 % à 40 % de l’énergie. The impact of digital technology on energy consumption is not limited to the future, February 2022.
-The physical constraints imposed by this manufacturing process are beyond Europe's reach, especially in Asia. This has a real revolution for manufacturers and forces them to rethink their environmental policies in order to compete.
-It is a radical transformation, but it is necessary in order to overtake digital technology and energy consumption. Les sociétés européennes doivent se montrer plus efficaces en matière de consommation d’énergie et de développement durable.
- Les systèmes informatiques devront être capables de réaliser des calculs complexes à une grande vitesse, sans consommation excessive d’énergie.
- La concentration des acteurs du secteur télécommunications est une menace pour l’innovation et la concurrence. Le document présente les différents projets qui concernent la fabrication et l’exploitation des matériels.
- Les changements européens sur l’ensemble des matériels seront aussi prend en compte la consommation d’énergie. L'Airbus est actif sur la totalité de la chaîne de production d'électronique, depuis les systèmes électriques (ST) jusqu'à l'assemblage et à la distribution (OVH).
- Cette activité lui donne une responsabilité en matière de leadership dans la construction d'une position commune pour toutes les technologies électroniques.
- La simulation va jusqu'à faire « voler au sol » les avions que l'on conçoit. Les gains en temps et en puissance des systèmes électroniques ont permis à Airbus de garder le contrôle de sa plateforme de service « Skyways » utilisée par beaucoup de compagnies aériennes. Les opérateurs énergétiques et leurs rapports avec l’enjeu de la décarbonation sont clairement identifiés comme un des grands problèmes à résoudre.
- Les fortes divergences entre les études sur ce sujet constituent une lacune importante qui doit être comblée. La France a des atouts, notamment résulte une mesure globale. 
- De la recherche sa présence sur l’ensemble de la chaîne du est nécessaire par exemple dans le domaine numérique. 
- Peu exploré des relations entre algorithmes, • Pousser l’European processeur initiative, en notamment algorithmes distribués, et se donnant pour 2030 l’objectif de 65–70 % consommations (où il est déjà évident que la d’approvisionnement européen ; quantité d’échanges peer to peer comme ceux • ainsi que l’initiative Euro HPC ; intervenant dans des algorithmes de bitcoin • partage européen des pratiques et standards ou de blockchain n’est pas généralisable, cette en cyber-sécurité. Encadrer la diversité quantité croissant comme le carré du nombre des transcriptions nationales des règles de participants) ; européennes ; • cette connaissance globale permettra • pousser le Climate Neutral Data Center Pact ; d’optimiser l’ensemble coût-performance- • Imposer la recyclabilité et interdire qualité environnementale des systèmes, l’obsolescence programmée. y compris les règles et protocoles d’échange de données, et en y intégrant notamment les data centers ; • et de concevoir des systèmes « juste assez », tenant compte au plus juste de l’ensemble des contraintes. France should take advantage of this new development by investing in research and development on new information technology technologies; 
- programmers who are well-versed in working with designers of hardware and software and can maintain the country's computer security should be trained; 
- redeveloping the country's application software to take advantage of the contributions of specialized components and mass parallelism will also be pursued; 
- networks of neurons, neural computation, and artificial intelligence will be studied for autonomous or shared vehicles (Pichereau report), without neglecting socio-economic support.